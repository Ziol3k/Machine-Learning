{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Understanding\n",
    "\n",
    "In this section we go through topics related to text understanding. We cover such topics like:\n",
    "    \n",
    "- Similarity measures\n",
    "- Word Vectors\n",
    "- Vector Space Model\n",
    "- Type of vectorizers\n",
    "- Build a vectorizer with Tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity measures\n",
    "\n",
    "Word does have different meanings. This makes the comparison and analysis a bit more complex."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from textblob import Word\n",
    "\n",
    "w = Word(\"developer\")\n",
    "\n",
    "for synset, definition in zip(w.get_synsets(), w.define()):\n",
    "    print(synset, definition)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity measures\n",
    "\n",
    "There are plenty of methods to measure the similarity of strings. Two most popular Python libraries examples for such measure are shown. We compare two strings: trains and training. The SequenceMatcher class allow us to use the Gestalt pattern matching algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from difflib import SequenceMatcher\n",
    "a = \"training\"\n",
    "b = \"trains\"\n",
    "print(len(a))\n",
    "print(len(b))\n",
    "ratio = SequenceMatcher(None, a, b).ratio()\n",
    "print(ratio)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distance is a normalized value between 0 and 1, where 1 means identical.\n",
    "\n",
    "A different approach is shown below. We use the Jellyfish library. There are a few methods that we can use here. One of it is the Levenshtein distance. Below the distance and normalize distance values are calculated."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import jellyfish\n",
    "distance = jellyfish.levenshtein_distance(a,b)\n",
    "print(distance)\n",
    "\n",
    "normalized_distance = distance/max(len(a),len(b))\n",
    "print(1.0-normalized_distance)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some words can be more similar to each other than other. We can build a similarity matrix to check it where 1 mean equal and 0 totally different."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "tokens = nlp(u'king queen horse cat desk lamp')\n",
    "\n",
    "for first_token in tokens:\n",
    "    for second_token in tokens:\n",
    "        print(first_token.text, second_token.text, first_token.similarity(second_token))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compare sentences:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "doc1 = nlp(u\"Warsaw is the largest city in Poland.\")\n",
    "doc2 = nlp(u\"Crossaint is baked in France.\")\n",
    "doc3 = nlp(u\"An emu is a large bird.\")\n",
    "\n",
    "for doc in [doc1, doc2, doc3]:\n",
    "    for other_doc in [doc1, doc2, doc3]:\n",
    "        print(doc.similarity(other_doc))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The similarity matrix looks like following:\n",
    "\n",
    "|       | doc1 | doc2 | doc3 |\n",
    "|-------|------|------|------|\n",
    "| **doc1** | 1.0  | 0.72 | 0.65 |\n",
    "| **doc 2** | 0.72 | 1.0  | 0.40 |\n",
    "| **doc 3** | 0.65 | 0.40 | 1.0  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors\n",
    "\n",
    "SpaCy does have already a set of words that are vectorized.\n",
    "\n",
    "Let's take a look at the vectors that are available in spaCy using the previous example:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "tokens = nlp(u'king queen horse cat desk lamp')\n",
    "\n",
    "for token in tokens:\n",
    "    print(str(token)+\" \"+str(token.vector))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks that the vectors are quite long. It's easy to check the exact size of a vector:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "len(tokens[1].vector)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can play around and check the vector values for some other sentences. Let's take a look at sentence vectors of one of our previous examples:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "len(doc1.vector)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice example of word vectorization done by some researchers at Warsaw University: [Word2Vec](https://lamyiowce.github.io/word2viz/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative sampling\n",
    "\n",
    "It is a simpler implementation of word2vec. It is faster as it takes only a few terms in each iteration for training insted of the whole dataset as in previous example. This is why it's called negative sampling.\n",
    "\n",
    "First of all, we define helper methods that are used later."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def zeros(*dims):\n",
    "    return np.zeros(shape=tuple(dims), dtype=np.float32)\n",
    "\n",
    "def ones(*dims):\n",
    "    return np.ones(shape=tuple(dims), dtype=np.float32)\n",
    "\n",
    "def rand(*dims):\n",
    "    return np.random.rand(*dims).astype(np.float32)\n",
    "\n",
    "def randn(*dims):\n",
    "    return np.random.randn(*dims).astype(np.float32)\n",
    "\n",
    "def sigmoid(batch, stochastic=False):\n",
    "    return  1.0 / (1.0 + np.exp(-batch))\n",
    "\n",
    "def as_matrix(vector):\n",
    "    return np.reshape(vector, (-1, 1))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to load the data again."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import namedtuple\n",
    "\n",
    "nltk.download('all')\n",
    "\n",
    "from nltk.book import *\n",
    "\n",
    "texts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three variables are important for the training: ``train_dict``, ``train_tokens`` and ``train_set``. The first one contain all unique words used in the corpus. The second is a list of indices of words in the dictionary that correspond to each word used in the raw text. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#raw_set = nltk.corpus.treebank_raw.raw()[0:50000].replace('.START',' ').replace(\"\\n\",\"\").replace(\".\",\" \").replace(\",\",\" \")\n",
    "#tokens = [token for token in nltk.word_tokenize(raw_set) if token.isalpha()]\n",
    "tokens = text6.tokens\n",
    "train_dict = pd.Series(tokens).unique().tolist()\n",
    "train_tokens = np.array([train_dict.index(token) for token in tokens])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last variable consist of a list of two numbers. The current word index and the word index that is before the word and after the word. Depending on the window size we use also other words that are in the neighbourhood. In this example the window size is set to 2. It means we take two words before and two words after the given word and build the relation in the training data set."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "train_set = []\n",
    "for i in range(2,len(tokens)-2):\n",
    "    train_set.append([train_dict.index(tokens[i]), train_dict.index(tokens[i-1])])\n",
    "    train_set.append([train_dict.index(tokens[i]), train_dict.index(tokens[i-2])])\n",
    "    train_set.append([train_dict.index(tokens[i]), train_dict.index(tokens[i+1])])\n",
    "    train_set.append([train_dict.index(tokens[i]), train_dict.index(tokens[i+2])])\n",
    "\n",
    "train_set = np.random.permutation(np.array(train_set))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to set the training configuration. We set the the negative samples size to 10 and the vector size to 100. Learning rate and rate decay are set to 0.1 and 0.995. The training loops are set to 8000000. Logs are displayed each 10000 epoches."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "Config = namedtuple(\"Config\", [\"dict_size\", \"vect_size\", \"neg_samples\", \"updates\", \"learning_rate\",\n",
    "                               \"learning_rate_decay\", \"decay_period\", \"log_period\"])\n",
    "conf = Config(\n",
    "    dict_size=len(train_dict),\n",
    "    vect_size=100,\n",
    "    neg_samples=10,\n",
    "    updates=8000000,\n",
    "    learning_rate=0.1,\n",
    "    learning_rate_decay=0.995,\n",
    "    decay_period=10000,\n",
    "    log_period=10000)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We loop over ``updates`` and get the word and context from the train set. We calculate the negative context and calculate the word, context and negative sample vectors. The negative context is chosen randomly. In the next step we calcualte the cost and corresponding to it gradients."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def neg_sample(conf, train_set, train_tokens):\n",
    "    Vp = randn(conf.dict_size, conf.vect_size)\n",
    "    Vo = randn(conf.dict_size, conf.vect_size)\n",
    "\n",
    "    J = 0.0\n",
    "    learning_rate = conf.learning_rate\n",
    "    for i in range(conf.updates):\n",
    "        idx = i % len(train_set)\n",
    "\n",
    "        word = train_set[idx, 0]\n",
    "        context = train_set[idx, 1]\n",
    "\n",
    "        neg_context = np.random.randint(0, len(train_tokens), conf.neg_samples)\n",
    "        neg_context = train_tokens[neg_context]\n",
    "\n",
    "        word_vect = Vp[word, :]  # word vector\n",
    "        context_vect = Vo[context, :];  # context wector\n",
    "        negative_vects = Vo[neg_context, :]  # sampled negative vectors\n",
    "\n",
    "        # Cost and gradient calculation starts here\n",
    "        score_pos = word_vect @ context_vect.T\n",
    "        score_neg = word_vect @ negative_vects.T\n",
    "\n",
    "        J -= np.log(sigmoid(score_pos)) + np.sum(np.log(sigmoid(-score_neg)))\n",
    "        if (i + 1) % conf.log_period == 0:\n",
    "            print('Update {0}\\tcost: {1:>2.2f}'.format(i + 1, J / conf.log_period))\n",
    "            final_cost = J / conf.log_period\n",
    "            J = 0.0\n",
    "\n",
    "        pos_g = 1.0 - sigmoid(score_pos)\n",
    "        neg_g = sigmoid(score_neg)\n",
    "\n",
    "        word_grad = -pos_g * context_vect + np.sum(as_matrix(neg_g) * negative_vects, axis=0)\n",
    "        context_grad = -pos_g * word_vect\n",
    "        neg_context_grad = as_matrix(neg_g) * as_matrix(word_vect).T\n",
    "\n",
    "        Vp[word, :] -= learning_rate * word_grad\n",
    "        Vo[context, :] -= learning_rate * context_grad\n",
    "        Vo[neg_context, :] -= learning_rate * neg_context_grad\n",
    "\n",
    "        if i % conf.decay_period == 0:\n",
    "            learning_rate = learning_rate * conf.learning_rate_decay\n",
    "\n",
    "    return Vp, Vo, final_cost"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next do the training:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "Vp, Vo, J = neg_sample(conf, train_set, train_tokens)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``similar_words`` can be used to find related words of the ``word``."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def lookup_word_idx(word, word_dict):\n",
    "    try:\n",
    "        return np.argwhere(np.array(word_dict) == word)[0][0]\n",
    "    except:\n",
    "        raise Exception(\"No such word in dict: {}\".format(word))\n",
    "\n",
    "def similar_words(embeddings, word, word_dict, hits):\n",
    "    word_idx = lookup_word_idx(word, word_dict)\n",
    "    similarity_scores = embeddings @ embeddings[word_idx]\n",
    "    similar_word_idxs = np.argsort(-similarity_scores)    \n",
    "    return [word_dict[i] for i in similar_word_idxs[:hits]]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print('\\n\\nTraining cost: {0:>2.2f}\\n\\n'.format(J))\n",
    "\n",
    "sample_words = ['knight', 'holy', 'grail']\n",
    "\n",
    "Vp_norm = Vp / as_matrix(np.linalg.norm(Vp , axis=1))\n",
    "for w in sample_words:\n",
    "    similar = similar_words(Vp_norm, w, train_dict, 5)\n",
    "    print('Words similar to {}: {}'.format(w, \", \".join(similar)))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    "[1] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
